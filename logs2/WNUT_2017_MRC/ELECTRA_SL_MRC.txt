Dataset: WNUT_2017
Model Name: ELECTRA_SL_MRC
Total Runs: 1
Learning Rate: 0.001
Fine-Tune Learning Rate: 2e-05
Mixed-Case Training: no
Display Step: 30
SEED base value: 101


Parameter Count: 334095375


layer_weights
torch.Size([12])
BigTransformer.embeddings.word_embeddings.weight
torch.Size([30522, 1024])
BigTransformer.embeddings.position_embeddings.weight
torch.Size([512, 1024])
BigTransformer.embeddings.token_type_embeddings.weight
torch.Size([2, 1024])
BigTransformer.embeddings.LayerNorm.weight
torch.Size([1024])
BigTransformer.embeddings.LayerNorm.bias
torch.Size([1024])
BigTransformer.encoder.layer.0.attention.self.query.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.0.attention.self.query.bias
torch.Size([1024])
BigTransformer.encoder.layer.0.attention.self.key.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.0.attention.self.key.bias
torch.Size([1024])
BigTransformer.encoder.layer.0.attention.self.value.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.0.attention.self.value.bias
torch.Size([1024])
BigTransformer.encoder.layer.0.attention.output.dense.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.0.attention.output.dense.bias
torch.Size([1024])
BigTransformer.encoder.layer.0.attention.output.LayerNorm.weight
torch.Size([1024])
BigTransformer.encoder.layer.0.attention.output.LayerNorm.bias
torch.Size([1024])
BigTransformer.encoder.layer.0.intermediate.dense.weight
torch.Size([4096, 1024])
BigTransformer.encoder.layer.0.intermediate.dense.bias
torch.Size([4096])
BigTransformer.encoder.layer.0.output.dense.weight
torch.Size([1024, 4096])
BigTransformer.encoder.layer.0.output.dense.bias
torch.Size([1024])
BigTransformer.encoder.layer.0.output.LayerNorm.weight
torch.Size([1024])
BigTransformer.encoder.layer.0.output.LayerNorm.bias
torch.Size([1024])
BigTransformer.encoder.layer.1.attention.self.query.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.1.attention.self.query.bias
torch.Size([1024])
BigTransformer.encoder.layer.1.attention.self.key.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.1.attention.self.key.bias
torch.Size([1024])
BigTransformer.encoder.layer.1.attention.self.value.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.1.attention.self.value.bias
torch.Size([1024])
BigTransformer.encoder.layer.1.attention.output.dense.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.1.attention.output.dense.bias
torch.Size([1024])
BigTransformer.encoder.layer.1.attention.output.LayerNorm.weight
torch.Size([1024])
BigTransformer.encoder.layer.1.attention.output.LayerNorm.bias
torch.Size([1024])
BigTransformer.encoder.layer.1.intermediate.dense.weight
torch.Size([4096, 1024])
BigTransformer.encoder.layer.1.intermediate.dense.bias
torch.Size([4096])
BigTransformer.encoder.layer.1.output.dense.weight
torch.Size([1024, 4096])
BigTransformer.encoder.layer.1.output.dense.bias
torch.Size([1024])
BigTransformer.encoder.layer.1.output.LayerNorm.weight
torch.Size([1024])
BigTransformer.encoder.layer.1.output.LayerNorm.bias
torch.Size([1024])
BigTransformer.encoder.layer.2.attention.self.query.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.2.attention.self.query.bias
torch.Size([1024])
BigTransformer.encoder.layer.2.attention.self.key.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.2.attention.self.key.bias
torch.Size([1024])
BigTransformer.encoder.layer.2.attention.self.value.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.2.attention.self.value.bias
torch.Size([1024])
BigTransformer.encoder.layer.2.attention.output.dense.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.2.attention.output.dense.bias
torch.Size([1024])
BigTransformer.encoder.layer.2.attention.output.LayerNorm.weight
torch.Size([1024])
BigTransformer.encoder.layer.2.attention.output.LayerNorm.bias
torch.Size([1024])
BigTransformer.encoder.layer.2.intermediate.dense.weight
torch.Size([4096, 1024])
BigTransformer.encoder.layer.2.intermediate.dense.bias
torch.Size([4096])
BigTransformer.encoder.layer.2.output.dense.weight
torch.Size([1024, 4096])
BigTransformer.encoder.layer.2.output.dense.bias
torch.Size([1024])
BigTransformer.encoder.layer.2.output.LayerNorm.weight
torch.Size([1024])
BigTransformer.encoder.layer.2.output.LayerNorm.bias
torch.Size([1024])
BigTransformer.encoder.layer.3.attention.self.query.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.3.attention.self.query.bias
torch.Size([1024])
BigTransformer.encoder.layer.3.attention.self.key.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.3.attention.self.key.bias
torch.Size([1024])
BigTransformer.encoder.layer.3.attention.self.value.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.3.attention.self.value.bias
torch.Size([1024])
BigTransformer.encoder.layer.3.attention.output.dense.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.3.attention.output.dense.bias
torch.Size([1024])
BigTransformer.encoder.layer.3.attention.output.LayerNorm.weight
torch.Size([1024])
BigTransformer.encoder.layer.3.attention.output.LayerNorm.bias
torch.Size([1024])
BigTransformer.encoder.layer.3.intermediate.dense.weight
torch.Size([4096, 1024])
BigTransformer.encoder.layer.3.intermediate.dense.bias
torch.Size([4096])
BigTransformer.encoder.layer.3.output.dense.weight
torch.Size([1024, 4096])
BigTransformer.encoder.layer.3.output.dense.bias
torch.Size([1024])
BigTransformer.encoder.layer.3.output.LayerNorm.weight
torch.Size([1024])
BigTransformer.encoder.layer.3.output.LayerNorm.bias
torch.Size([1024])
BigTransformer.encoder.layer.4.attention.self.query.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.4.attention.self.query.bias
torch.Size([1024])
BigTransformer.encoder.layer.4.attention.self.key.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.4.attention.self.key.bias
torch.Size([1024])
BigTransformer.encoder.layer.4.attention.self.value.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.4.attention.self.value.bias
torch.Size([1024])
BigTransformer.encoder.layer.4.attention.output.dense.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.4.attention.output.dense.bias
torch.Size([1024])
BigTransformer.encoder.layer.4.attention.output.LayerNorm.weight
torch.Size([1024])
BigTransformer.encoder.layer.4.attention.output.LayerNorm.bias
torch.Size([1024])
BigTransformer.encoder.layer.4.intermediate.dense.weight
torch.Size([4096, 1024])
BigTransformer.encoder.layer.4.intermediate.dense.bias
torch.Size([4096])
BigTransformer.encoder.layer.4.output.dense.weight
torch.Size([1024, 4096])
BigTransformer.encoder.layer.4.output.dense.bias
torch.Size([1024])
BigTransformer.encoder.layer.4.output.LayerNorm.weight
torch.Size([1024])
BigTransformer.encoder.layer.4.output.LayerNorm.bias
torch.Size([1024])
BigTransformer.encoder.layer.5.attention.self.query.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.5.attention.self.query.bias
torch.Size([1024])
BigTransformer.encoder.layer.5.attention.self.key.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.5.attention.self.key.bias
torch.Size([1024])
BigTransformer.encoder.layer.5.attention.self.value.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.5.attention.self.value.bias
torch.Size([1024])
BigTransformer.encoder.layer.5.attention.output.dense.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.5.attention.output.dense.bias
torch.Size([1024])
BigTransformer.encoder.layer.5.attention.output.LayerNorm.weight
torch.Size([1024])
BigTransformer.encoder.layer.5.attention.output.LayerNorm.bias
torch.Size([1024])
BigTransformer.encoder.layer.5.intermediate.dense.weight
torch.Size([4096, 1024])
BigTransformer.encoder.layer.5.intermediate.dense.bias
torch.Size([4096])
BigTransformer.encoder.layer.5.output.dense.weight
torch.Size([1024, 4096])
BigTransformer.encoder.layer.5.output.dense.bias
torch.Size([1024])
BigTransformer.encoder.layer.5.output.LayerNorm.weight
torch.Size([1024])
BigTransformer.encoder.layer.5.output.LayerNorm.bias
torch.Size([1024])
BigTransformer.encoder.layer.6.attention.self.query.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.6.attention.self.query.bias
torch.Size([1024])
BigTransformer.encoder.layer.6.attention.self.key.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.6.attention.self.key.bias
torch.Size([1024])
BigTransformer.encoder.layer.6.attention.self.value.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.6.attention.self.value.bias
torch.Size([1024])
BigTransformer.encoder.layer.6.attention.output.dense.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.6.attention.output.dense.bias
torch.Size([1024])
BigTransformer.encoder.layer.6.attention.output.LayerNorm.weight
torch.Size([1024])
BigTransformer.encoder.layer.6.attention.output.LayerNorm.bias
torch.Size([1024])
BigTransformer.encoder.layer.6.intermediate.dense.weight
torch.Size([4096, 1024])
BigTransformer.encoder.layer.6.intermediate.dense.bias
torch.Size([4096])
BigTransformer.encoder.layer.6.output.dense.weight
torch.Size([1024, 4096])
BigTransformer.encoder.layer.6.output.dense.bias
torch.Size([1024])
BigTransformer.encoder.layer.6.output.LayerNorm.weight
torch.Size([1024])
BigTransformer.encoder.layer.6.output.LayerNorm.bias
torch.Size([1024])
BigTransformer.encoder.layer.7.attention.self.query.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.7.attention.self.query.bias
torch.Size([1024])
BigTransformer.encoder.layer.7.attention.self.key.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.7.attention.self.key.bias
torch.Size([1024])
BigTransformer.encoder.layer.7.attention.self.value.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.7.attention.self.value.bias
torch.Size([1024])
BigTransformer.encoder.layer.7.attention.output.dense.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.7.attention.output.dense.bias
torch.Size([1024])
BigTransformer.encoder.layer.7.attention.output.LayerNorm.weight
torch.Size([1024])
BigTransformer.encoder.layer.7.attention.output.LayerNorm.bias
torch.Size([1024])
BigTransformer.encoder.layer.7.intermediate.dense.weight
torch.Size([4096, 1024])
BigTransformer.encoder.layer.7.intermediate.dense.bias
torch.Size([4096])
BigTransformer.encoder.layer.7.output.dense.weight
torch.Size([1024, 4096])
BigTransformer.encoder.layer.7.output.dense.bias
torch.Size([1024])
BigTransformer.encoder.layer.7.output.LayerNorm.weight
torch.Size([1024])
BigTransformer.encoder.layer.7.output.LayerNorm.bias
torch.Size([1024])
BigTransformer.encoder.layer.8.attention.self.query.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.8.attention.self.query.bias
torch.Size([1024])
BigTransformer.encoder.layer.8.attention.self.key.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.8.attention.self.key.bias
torch.Size([1024])
BigTransformer.encoder.layer.8.attention.self.value.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.8.attention.self.value.bias
torch.Size([1024])
BigTransformer.encoder.layer.8.attention.output.dense.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.8.attention.output.dense.bias
torch.Size([1024])
BigTransformer.encoder.layer.8.attention.output.LayerNorm.weight
torch.Size([1024])
BigTransformer.encoder.layer.8.attention.output.LayerNorm.bias
torch.Size([1024])
BigTransformer.encoder.layer.8.intermediate.dense.weight
torch.Size([4096, 1024])
BigTransformer.encoder.layer.8.intermediate.dense.bias
torch.Size([4096])
BigTransformer.encoder.layer.8.output.dense.weight
torch.Size([1024, 4096])
BigTransformer.encoder.layer.8.output.dense.bias
torch.Size([1024])
BigTransformer.encoder.layer.8.output.LayerNorm.weight
torch.Size([1024])
BigTransformer.encoder.layer.8.output.LayerNorm.bias
torch.Size([1024])
BigTransformer.encoder.layer.9.attention.self.query.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.9.attention.self.query.bias
torch.Size([1024])
BigTransformer.encoder.layer.9.attention.self.key.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.9.attention.self.key.bias
torch.Size([1024])
BigTransformer.encoder.layer.9.attention.self.value.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.9.attention.self.value.bias
torch.Size([1024])
BigTransformer.encoder.layer.9.attention.output.dense.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.9.attention.output.dense.bias
torch.Size([1024])
BigTransformer.encoder.layer.9.attention.output.LayerNorm.weight
torch.Size([1024])
BigTransformer.encoder.layer.9.attention.output.LayerNorm.bias
torch.Size([1024])
BigTransformer.encoder.layer.9.intermediate.dense.weight
torch.Size([4096, 1024])
BigTransformer.encoder.layer.9.intermediate.dense.bias
torch.Size([4096])
BigTransformer.encoder.layer.9.output.dense.weight
torch.Size([1024, 4096])
BigTransformer.encoder.layer.9.output.dense.bias
torch.Size([1024])
BigTransformer.encoder.layer.9.output.LayerNorm.weight
torch.Size([1024])
BigTransformer.encoder.layer.9.output.LayerNorm.bias
torch.Size([1024])
BigTransformer.encoder.layer.10.attention.self.query.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.10.attention.self.query.bias
torch.Size([1024])
BigTransformer.encoder.layer.10.attention.self.key.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.10.attention.self.key.bias
torch.Size([1024])
BigTransformer.encoder.layer.10.attention.self.value.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.10.attention.self.value.bias
torch.Size([1024])
BigTransformer.encoder.layer.10.attention.output.dense.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.10.attention.output.dense.bias
torch.Size([1024])
BigTransformer.encoder.layer.10.attention.output.LayerNorm.weight
torch.Size([1024])
BigTransformer.encoder.layer.10.attention.output.LayerNorm.bias
torch.Size([1024])
BigTransformer.encoder.layer.10.intermediate.dense.weight
torch.Size([4096, 1024])
BigTransformer.encoder.layer.10.intermediate.dense.bias
torch.Size([4096])
BigTransformer.encoder.layer.10.output.dense.weight
torch.Size([1024, 4096])
BigTransformer.encoder.layer.10.output.dense.bias
torch.Size([1024])
BigTransformer.encoder.layer.10.output.LayerNorm.weight
torch.Size([1024])
BigTransformer.encoder.layer.10.output.LayerNorm.bias
torch.Size([1024])
BigTransformer.encoder.layer.11.attention.self.query.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.11.attention.self.query.bias
torch.Size([1024])
BigTransformer.encoder.layer.11.attention.self.key.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.11.attention.self.key.bias
torch.Size([1024])
BigTransformer.encoder.layer.11.attention.self.value.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.11.attention.self.value.bias
torch.Size([1024])
BigTransformer.encoder.layer.11.attention.output.dense.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.11.attention.output.dense.bias
torch.Size([1024])
BigTransformer.encoder.layer.11.attention.output.LayerNorm.weight
torch.Size([1024])
BigTransformer.encoder.layer.11.attention.output.LayerNorm.bias
torch.Size([1024])
BigTransformer.encoder.layer.11.intermediate.dense.weight
torch.Size([4096, 1024])
BigTransformer.encoder.layer.11.intermediate.dense.bias
torch.Size([4096])
BigTransformer.encoder.layer.11.output.dense.weight
torch.Size([1024, 4096])
BigTransformer.encoder.layer.11.output.dense.bias
torch.Size([1024])
BigTransformer.encoder.layer.11.output.LayerNorm.weight
torch.Size([1024])
BigTransformer.encoder.layer.11.output.LayerNorm.bias
torch.Size([1024])
BigTransformer.encoder.layer.12.attention.self.query.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.12.attention.self.query.bias
torch.Size([1024])
BigTransformer.encoder.layer.12.attention.self.key.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.12.attention.self.key.bias
torch.Size([1024])
BigTransformer.encoder.layer.12.attention.self.value.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.12.attention.self.value.bias
torch.Size([1024])
BigTransformer.encoder.layer.12.attention.output.dense.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.12.attention.output.dense.bias
torch.Size([1024])
BigTransformer.encoder.layer.12.attention.output.LayerNorm.weight
torch.Size([1024])
BigTransformer.encoder.layer.12.attention.output.LayerNorm.bias
torch.Size([1024])
BigTransformer.encoder.layer.12.intermediate.dense.weight
torch.Size([4096, 1024])
BigTransformer.encoder.layer.12.intermediate.dense.bias
torch.Size([4096])
BigTransformer.encoder.layer.12.output.dense.weight
torch.Size([1024, 4096])
BigTransformer.encoder.layer.12.output.dense.bias
torch.Size([1024])
BigTransformer.encoder.layer.12.output.LayerNorm.weight
torch.Size([1024])
BigTransformer.encoder.layer.12.output.LayerNorm.bias
torch.Size([1024])
BigTransformer.encoder.layer.13.attention.self.query.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.13.attention.self.query.bias
torch.Size([1024])
BigTransformer.encoder.layer.13.attention.self.key.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.13.attention.self.key.bias
torch.Size([1024])
BigTransformer.encoder.layer.13.attention.self.value.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.13.attention.self.value.bias
torch.Size([1024])
BigTransformer.encoder.layer.13.attention.output.dense.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.13.attention.output.dense.bias
torch.Size([1024])
BigTransformer.encoder.layer.13.attention.output.LayerNorm.weight
torch.Size([1024])
BigTransformer.encoder.layer.13.attention.output.LayerNorm.bias
torch.Size([1024])
BigTransformer.encoder.layer.13.intermediate.dense.weight
torch.Size([4096, 1024])
BigTransformer.encoder.layer.13.intermediate.dense.bias
torch.Size([4096])
BigTransformer.encoder.layer.13.output.dense.weight
torch.Size([1024, 4096])
BigTransformer.encoder.layer.13.output.dense.bias
torch.Size([1024])
BigTransformer.encoder.layer.13.output.LayerNorm.weight
torch.Size([1024])
BigTransformer.encoder.layer.13.output.LayerNorm.bias
torch.Size([1024])
BigTransformer.encoder.layer.14.attention.self.query.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.14.attention.self.query.bias
torch.Size([1024])
BigTransformer.encoder.layer.14.attention.self.key.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.14.attention.self.key.bias
torch.Size([1024])
BigTransformer.encoder.layer.14.attention.self.value.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.14.attention.self.value.bias
torch.Size([1024])
BigTransformer.encoder.layer.14.attention.output.dense.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.14.attention.output.dense.bias
torch.Size([1024])
BigTransformer.encoder.layer.14.attention.output.LayerNorm.weight
torch.Size([1024])
BigTransformer.encoder.layer.14.attention.output.LayerNorm.bias
torch.Size([1024])
BigTransformer.encoder.layer.14.intermediate.dense.weight
torch.Size([4096, 1024])
BigTransformer.encoder.layer.14.intermediate.dense.bias
torch.Size([4096])
BigTransformer.encoder.layer.14.output.dense.weight
torch.Size([1024, 4096])
BigTransformer.encoder.layer.14.output.dense.bias
torch.Size([1024])
BigTransformer.encoder.layer.14.output.LayerNorm.weight
torch.Size([1024])
BigTransformer.encoder.layer.14.output.LayerNorm.bias
torch.Size([1024])
BigTransformer.encoder.layer.15.attention.self.query.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.15.attention.self.query.bias
torch.Size([1024])
BigTransformer.encoder.layer.15.attention.self.key.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.15.attention.self.key.bias
torch.Size([1024])
BigTransformer.encoder.layer.15.attention.self.value.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.15.attention.self.value.bias
torch.Size([1024])
BigTransformer.encoder.layer.15.attention.output.dense.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.15.attention.output.dense.bias
torch.Size([1024])
BigTransformer.encoder.layer.15.attention.output.LayerNorm.weight
torch.Size([1024])
BigTransformer.encoder.layer.15.attention.output.LayerNorm.bias
torch.Size([1024])
BigTransformer.encoder.layer.15.intermediate.dense.weight
torch.Size([4096, 1024])
BigTransformer.encoder.layer.15.intermediate.dense.bias
torch.Size([4096])
BigTransformer.encoder.layer.15.output.dense.weight
torch.Size([1024, 4096])
BigTransformer.encoder.layer.15.output.dense.bias
torch.Size([1024])
BigTransformer.encoder.layer.15.output.LayerNorm.weight
torch.Size([1024])
BigTransformer.encoder.layer.15.output.LayerNorm.bias
torch.Size([1024])
BigTransformer.encoder.layer.16.attention.self.query.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.16.attention.self.query.bias
torch.Size([1024])
BigTransformer.encoder.layer.16.attention.self.key.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.16.attention.self.key.bias
torch.Size([1024])
BigTransformer.encoder.layer.16.attention.self.value.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.16.attention.self.value.bias
torch.Size([1024])
BigTransformer.encoder.layer.16.attention.output.dense.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.16.attention.output.dense.bias
torch.Size([1024])
BigTransformer.encoder.layer.16.attention.output.LayerNorm.weight
torch.Size([1024])
BigTransformer.encoder.layer.16.attention.output.LayerNorm.bias
torch.Size([1024])
BigTransformer.encoder.layer.16.intermediate.dense.weight
torch.Size([4096, 1024])
BigTransformer.encoder.layer.16.intermediate.dense.bias
torch.Size([4096])
BigTransformer.encoder.layer.16.output.dense.weight
torch.Size([1024, 4096])
BigTransformer.encoder.layer.16.output.dense.bias
torch.Size([1024])
BigTransformer.encoder.layer.16.output.LayerNorm.weight
torch.Size([1024])
BigTransformer.encoder.layer.16.output.LayerNorm.bias
torch.Size([1024])
BigTransformer.encoder.layer.17.attention.self.query.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.17.attention.self.query.bias
torch.Size([1024])
BigTransformer.encoder.layer.17.attention.self.key.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.17.attention.self.key.bias
torch.Size([1024])
BigTransformer.encoder.layer.17.attention.self.value.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.17.attention.self.value.bias
torch.Size([1024])
BigTransformer.encoder.layer.17.attention.output.dense.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.17.attention.output.dense.bias
torch.Size([1024])
BigTransformer.encoder.layer.17.attention.output.LayerNorm.weight
torch.Size([1024])
BigTransformer.encoder.layer.17.attention.output.LayerNorm.bias
torch.Size([1024])
BigTransformer.encoder.layer.17.intermediate.dense.weight
torch.Size([4096, 1024])
BigTransformer.encoder.layer.17.intermediate.dense.bias
torch.Size([4096])
BigTransformer.encoder.layer.17.output.dense.weight
torch.Size([1024, 4096])
BigTransformer.encoder.layer.17.output.dense.bias
torch.Size([1024])
BigTransformer.encoder.layer.17.output.LayerNorm.weight
torch.Size([1024])
BigTransformer.encoder.layer.17.output.LayerNorm.bias
torch.Size([1024])
BigTransformer.encoder.layer.18.attention.self.query.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.18.attention.self.query.bias
torch.Size([1024])
BigTransformer.encoder.layer.18.attention.self.key.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.18.attention.self.key.bias
torch.Size([1024])
BigTransformer.encoder.layer.18.attention.self.value.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.18.attention.self.value.bias
torch.Size([1024])
BigTransformer.encoder.layer.18.attention.output.dense.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.18.attention.output.dense.bias
torch.Size([1024])
BigTransformer.encoder.layer.18.attention.output.LayerNorm.weight
torch.Size([1024])
BigTransformer.encoder.layer.18.attention.output.LayerNorm.bias
torch.Size([1024])
BigTransformer.encoder.layer.18.intermediate.dense.weight
torch.Size([4096, 1024])
BigTransformer.encoder.layer.18.intermediate.dense.bias
torch.Size([4096])
BigTransformer.encoder.layer.18.output.dense.weight
torch.Size([1024, 4096])
BigTransformer.encoder.layer.18.output.dense.bias
torch.Size([1024])
BigTransformer.encoder.layer.18.output.LayerNorm.weight
torch.Size([1024])
BigTransformer.encoder.layer.18.output.LayerNorm.bias
torch.Size([1024])
BigTransformer.encoder.layer.19.attention.self.query.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.19.attention.self.query.bias
torch.Size([1024])
BigTransformer.encoder.layer.19.attention.self.key.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.19.attention.self.key.bias
torch.Size([1024])
BigTransformer.encoder.layer.19.attention.self.value.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.19.attention.self.value.bias
torch.Size([1024])
BigTransformer.encoder.layer.19.attention.output.dense.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.19.attention.output.dense.bias
torch.Size([1024])
BigTransformer.encoder.layer.19.attention.output.LayerNorm.weight
torch.Size([1024])
BigTransformer.encoder.layer.19.attention.output.LayerNorm.bias
torch.Size([1024])
BigTransformer.encoder.layer.19.intermediate.dense.weight
torch.Size([4096, 1024])
BigTransformer.encoder.layer.19.intermediate.dense.bias
torch.Size([4096])
BigTransformer.encoder.layer.19.output.dense.weight
torch.Size([1024, 4096])
BigTransformer.encoder.layer.19.output.dense.bias
torch.Size([1024])
BigTransformer.encoder.layer.19.output.LayerNorm.weight
torch.Size([1024])
BigTransformer.encoder.layer.19.output.LayerNorm.bias
torch.Size([1024])
BigTransformer.encoder.layer.20.attention.self.query.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.20.attention.self.query.bias
torch.Size([1024])
BigTransformer.encoder.layer.20.attention.self.key.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.20.attention.self.key.bias
torch.Size([1024])
BigTransformer.encoder.layer.20.attention.self.value.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.20.attention.self.value.bias
torch.Size([1024])
BigTransformer.encoder.layer.20.attention.output.dense.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.20.attention.output.dense.bias
torch.Size([1024])
BigTransformer.encoder.layer.20.attention.output.LayerNorm.weight
torch.Size([1024])
BigTransformer.encoder.layer.20.attention.output.LayerNorm.bias
torch.Size([1024])
BigTransformer.encoder.layer.20.intermediate.dense.weight
torch.Size([4096, 1024])
BigTransformer.encoder.layer.20.intermediate.dense.bias
torch.Size([4096])
BigTransformer.encoder.layer.20.output.dense.weight
torch.Size([1024, 4096])
BigTransformer.encoder.layer.20.output.dense.bias
torch.Size([1024])
BigTransformer.encoder.layer.20.output.LayerNorm.weight
torch.Size([1024])
BigTransformer.encoder.layer.20.output.LayerNorm.bias
torch.Size([1024])
BigTransformer.encoder.layer.21.attention.self.query.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.21.attention.self.query.bias
torch.Size([1024])
BigTransformer.encoder.layer.21.attention.self.key.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.21.attention.self.key.bias
torch.Size([1024])
BigTransformer.encoder.layer.21.attention.self.value.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.21.attention.self.value.bias
torch.Size([1024])
BigTransformer.encoder.layer.21.attention.output.dense.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.21.attention.output.dense.bias
torch.Size([1024])
BigTransformer.encoder.layer.21.attention.output.LayerNorm.weight
torch.Size([1024])
BigTransformer.encoder.layer.21.attention.output.LayerNorm.bias
torch.Size([1024])
BigTransformer.encoder.layer.21.intermediate.dense.weight
torch.Size([4096, 1024])
BigTransformer.encoder.layer.21.intermediate.dense.bias
torch.Size([4096])
BigTransformer.encoder.layer.21.output.dense.weight
torch.Size([1024, 4096])
BigTransformer.encoder.layer.21.output.dense.bias
torch.Size([1024])
BigTransformer.encoder.layer.21.output.LayerNorm.weight
torch.Size([1024])
BigTransformer.encoder.layer.21.output.LayerNorm.bias
torch.Size([1024])
BigTransformer.encoder.layer.22.attention.self.query.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.22.attention.self.query.bias
torch.Size([1024])
BigTransformer.encoder.layer.22.attention.self.key.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.22.attention.self.key.bias
torch.Size([1024])
BigTransformer.encoder.layer.22.attention.self.value.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.22.attention.self.value.bias
torch.Size([1024])
BigTransformer.encoder.layer.22.attention.output.dense.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.22.attention.output.dense.bias
torch.Size([1024])
BigTransformer.encoder.layer.22.attention.output.LayerNorm.weight
torch.Size([1024])
BigTransformer.encoder.layer.22.attention.output.LayerNorm.bias
torch.Size([1024])
BigTransformer.encoder.layer.22.intermediate.dense.weight
torch.Size([4096, 1024])
BigTransformer.encoder.layer.22.intermediate.dense.bias
torch.Size([4096])
BigTransformer.encoder.layer.22.output.dense.weight
torch.Size([1024, 4096])
BigTransformer.encoder.layer.22.output.dense.bias
torch.Size([1024])
BigTransformer.encoder.layer.22.output.LayerNorm.weight
torch.Size([1024])
BigTransformer.encoder.layer.22.output.LayerNorm.bias
torch.Size([1024])
BigTransformer.encoder.layer.23.attention.self.query.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.23.attention.self.query.bias
torch.Size([1024])
BigTransformer.encoder.layer.23.attention.self.key.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.23.attention.self.key.bias
torch.Size([1024])
BigTransformer.encoder.layer.23.attention.self.value.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.23.attention.self.value.bias
torch.Size([1024])
BigTransformer.encoder.layer.23.attention.output.dense.weight
torch.Size([1024, 1024])
BigTransformer.encoder.layer.23.attention.output.dense.bias
torch.Size([1024])
BigTransformer.encoder.layer.23.attention.output.LayerNorm.weight
torch.Size([1024])
BigTransformer.encoder.layer.23.attention.output.LayerNorm.bias
torch.Size([1024])
BigTransformer.encoder.layer.23.intermediate.dense.weight
torch.Size([4096, 1024])
BigTransformer.encoder.layer.23.intermediate.dense.bias
torch.Size([4096])
BigTransformer.encoder.layer.23.output.dense.weight
torch.Size([1024, 4096])
BigTransformer.encoder.layer.23.output.dense.bias
torch.Size([1024])
BigTransformer.encoder.layer.23.output.LayerNorm.weight
torch.Size([1024])
BigTransformer.encoder.layer.23.output.LayerNorm.bias
torch.Size([1024])
node_potentials.weight
torch.Size([3, 1024])
node_potentials.bias
torch.Size([3])



RUN: 0




TRAINING

Model: (ELECTRA-SL-MRC), Epoch:   0, Iter:     0, Loss: 0.587, F1: 0.016
Model: (ELECTRA-SL-MRC), Epoch:   0, Iter:    30, Loss: 0.018, F1: 0.000
Model: (ELECTRA-SL-MRC), Epoch:   0, Iter:    60, Loss: 0.094, F1: 0.000
Model: (ELECTRA-SL-MRC), Epoch:   0, Iter:    90, Loss: 0.088, F1: 0.000
Model: (ELECTRA-SL-MRC), Epoch:   0, Iter:   120, Loss: 0.039, F1: 0.000
Model: (ELECTRA-SL-MRC), Epoch:   0, Iter:   150, Loss: 0.030, F1: 0.500
Model: (ELECTRA-SL-MRC), Epoch:   0, Iter:   180, Loss: 0.037, F1: 0.182
Model: (ELECTRA-SL-MRC), Epoch:   0, Iter:   210, Loss: 0.032, F1: 0.533
Model: (ELECTRA-SL-MRC), Epoch:   0, Iter:   240, Loss: 0.012, F1: 0.667
Model: (ELECTRA-SL-MRC), Epoch:   0, Iter:   270, Loss: 0.041, F1: 0.632
Model: (ELECTRA-SL-MRC), Epoch:   0, Iter:   300, Loss: 0.031, F1: 0.667
Model: (ELECTRA-SL-MRC), Epoch:   0, Iter:   330, Loss: 0.003, F1: 0.000



VALIDATING

Model: (ELECTRA-SL-MRC), Epoch:   0, Iter:     0, Loss: 0.017
Model: (ELECTRA-SL-MRC), Epoch:   0, Iter:    30, Loss: 0.047
Model: (ELECTRA-SL-MRC), Epoch:   0, Iter:    60, Loss: 0.039
Model: (ELECTRA-SL-MRC), Epoch:   0, Iter:    90, Loss: 0.060
Model: (ELECTRA-SL-MRC), Epoch:   0, Iter:   120, Loss: 0.091
Model: (ELECTRA-SL-MRC), Epoch:   0, Iter:   150, Loss: 0.020
Model: (ELECTRA-SL-MRC), Epoch:   0, Iter:   180, Loss: 0.072
Model: (ELECTRA-SL-MRC), Epoch:   0, Iter:   210, Loss: 0.033
Model: (ELECTRA-SL-MRC), Epoch:   0, Iter:   240, Loss: 0.027
Model: (ELECTRA-SL-MRC), Epoch:   0, Iter:   270, Loss: 0.025
Model: (ELECTRA-SL-MRC), Epoch:   0, Iter:   300, Loss: 0.014
Model: (ELECTRA-SL-MRC), Epoch:   0, Iter:   330, Loss: 0.149
Model: (ELECTRA-SL-MRC), Epoch:   0, Iter:   360, Loss: 0.015



TESTING

Model: (ELECTRA-SL-MRC), Epoch:   0, Iter:     0, Loss: 0.058
Model: (ELECTRA-SL-MRC), Epoch:   0, Iter:    30, Loss: 0.037
Model: (ELECTRA-SL-MRC), Epoch:   0, Iter:    60, Loss: 0.040
Model: (ELECTRA-SL-MRC), Epoch:   0, Iter:    90, Loss: 0.055
Model: (ELECTRA-SL-MRC), Epoch:   0, Iter:   120, Loss: 0.049
Model: (ELECTRA-SL-MRC), Epoch:   0, Iter:   150, Loss: 0.015
Model: (ELECTRA-SL-MRC), Epoch:   0, Iter:   180, Loss: 0.056
Model: (ELECTRA-SL-MRC), Epoch:   0, Iter:   210, Loss: 0.046
Model: (ELECTRA-SL-MRC), Epoch:   0, Iter:   240, Loss: 0.216
Model: (ELECTRA-SL-MRC), Epoch:   0, Iter:   270, Loss: 0.036
Model: (ELECTRA-SL-MRC), Epoch:   0, Iter:   300, Loss: 0.048
Model: (ELECTRA-SL-MRC), Epoch:   0, Iter:   330, Loss: 0.022
Model: (ELECTRA-SL-MRC), Epoch:   0, Iter:   360, Loss: 0.045
Model: (ELECTRA-SL-MRC), Epoch:   0, Iter:   390, Loss: 0.026
Model: (ELECTRA-SL-MRC), Epoch:   0, Iter:   420, Loss: 0.074
Model: (ELECTRA-SL-MRC), Epoch:   0, Iter:   450, Loss: 0.057
Model: (ELECTRA-SL-MRC), Epoch:   0, Iter:   480, Loss: 0.019



EPOCH SUMMARY

Model: (ELECTRA-SL-MRC), Epoch:   0, Mean Train Loss: 0.110, Mean Train F1: 0.280
Model: (ELECTRA-SL-MRC), Epoch:   0, Mean Validation Loss: 0.067, Validation F1: 0.460
Model: (ELECTRA-SL-MRC), Epoch:   0, Mean Test Loss: 0.060, Test F1: 0.379
Impatience Level: 0


Checkpoint Created!


TRAINING

Model: (ELECTRA-SL-MRC), Epoch:   1, Iter:     0, Loss: 0.033, F1: 0.471
Model: (ELECTRA-SL-MRC), Epoch:   1, Iter:    30, Loss: 0.042, F1: 0.462
Model: (ELECTRA-SL-MRC), Epoch:   1, Iter:    60, Loss: 0.007, F1: 0.857
Model: (ELECTRA-SL-MRC), Epoch:   1, Iter:    90, Loss: 0.017, F1: 0.750
Model: (ELECTRA-SL-MRC), Epoch:   1, Iter:   120, Loss: 0.017, F1: 0.545
Model: (ELECTRA-SL-MRC), Epoch:   1, Iter:   150, Loss: 0.023, F1: 0.444
Model: (ELECTRA-SL-MRC), Epoch:   1, Iter:   180, Loss: 0.066, F1: 0.375
Model: (ELECTRA-SL-MRC), Epoch:   1, Iter:   210, Loss: 0.032, F1: 0.500
Model: (ELECTRA-SL-MRC), Epoch:   1, Iter:   240, Loss: 0.005, F1: 0.933
Model: (ELECTRA-SL-MRC), Epoch:   1, Iter:   270, Loss: 0.008, F1: 0.909
Model: (ELECTRA-SL-MRC), Epoch:   1, Iter:   300, Loss: 0.013, F1: 0.769
Model: (ELECTRA-SL-MRC), Epoch:   1, Iter:   330, Loss: 0.012, F1: 0.933



VALIDATING

Model: (ELECTRA-SL-MRC), Epoch:   1, Iter:     0, Loss: 0.004
Model: (ELECTRA-SL-MRC), Epoch:   1, Iter:    30, Loss: 0.042
Model: (ELECTRA-SL-MRC), Epoch:   1, Iter:    60, Loss: 0.097
Model: (ELECTRA-SL-MRC), Epoch:   1, Iter:    90, Loss: 0.028
Model: (ELECTRA-SL-MRC), Epoch:   1, Iter:   120, Loss: 0.138
Model: (ELECTRA-SL-MRC), Epoch:   1, Iter:   150, Loss: 0.010
Model: (ELECTRA-SL-MRC), Epoch:   1, Iter:   180, Loss: 0.038
Model: (ELECTRA-SL-MRC), Epoch:   1, Iter:   210, Loss: 0.020
Model: (ELECTRA-SL-MRC), Epoch:   1, Iter:   240, Loss: 0.016
Model: (ELECTRA-SL-MRC), Epoch:   1, Iter:   270, Loss: 0.107
Model: (ELECTRA-SL-MRC), Epoch:   1, Iter:   300, Loss: 0.004
Model: (ELECTRA-SL-MRC), Epoch:   1, Iter:   330, Loss: 0.034
Model: (ELECTRA-SL-MRC), Epoch:   1, Iter:   360, Loss: 0.033



TESTING

Model: (ELECTRA-SL-MRC), Epoch:   1, Iter:     0, Loss: 0.005
Model: (ELECTRA-SL-MRC), Epoch:   1, Iter:    30, Loss: 0.020
Model: (ELECTRA-SL-MRC), Epoch:   1, Iter:    60, Loss: 0.003
Model: (ELECTRA-SL-MRC), Epoch:   1, Iter:    90, Loss: 0.004
Model: (ELECTRA-SL-MRC), Epoch:   1, Iter:   120, Loss: 0.066
Model: (ELECTRA-SL-MRC), Epoch:   1, Iter:   150, Loss: 0.176
Model: (ELECTRA-SL-MRC), Epoch:   1, Iter:   180, Loss: 0.014
Model: (ELECTRA-SL-MRC), Epoch:   1, Iter:   210, Loss: 0.006
Model: (ELECTRA-SL-MRC), Epoch:   1, Iter:   240, Loss: 0.040
Model: (ELECTRA-SL-MRC), Epoch:   1, Iter:   270, Loss: 0.091
Model: (ELECTRA-SL-MRC), Epoch:   1, Iter:   300, Loss: 0.146
Model: (ELECTRA-SL-MRC), Epoch:   1, Iter:   330, Loss: 0.030
Model: (ELECTRA-SL-MRC), Epoch:   1, Iter:   360, Loss: 0.041
Model: (ELECTRA-SL-MRC), Epoch:   1, Iter:   390, Loss: 0.015
Model: (ELECTRA-SL-MRC), Epoch:   1, Iter:   420, Loss: 0.032
Model: (ELECTRA-SL-MRC), Epoch:   1, Iter:   450, Loss: 0.006
Model: (ELECTRA-SL-MRC), Epoch:   1, Iter:   480, Loss: 0.021



EPOCH SUMMARY

Model: (ELECTRA-SL-MRC), Epoch:   1, Mean Train Loss: 0.045, Mean Train F1: 0.628
Model: (ELECTRA-SL-MRC), Epoch:   1, Mean Validation Loss: 0.047, Validation F1: 0.606
Model: (ELECTRA-SL-MRC), Epoch:   1, Mean Test Loss: 0.049, Test F1: 0.491
Impatience Level: 0


Checkpoint Created!


TRAINING

Model: (ELECTRA-SL-MRC), Epoch:   2, Iter:     0, Loss: 0.023, F1: 0.600
Model: (ELECTRA-SL-MRC), Epoch:   2, Iter:    30, Loss: 0.015, F1: 0.800
Model: (ELECTRA-SL-MRC), Epoch:   2, Iter:    60, Loss: 0.008, F1: 1.000
Model: (ELECTRA-SL-MRC), Epoch:   2, Iter:    90, Loss: 0.012, F1: 0.667
Model: (ELECTRA-SL-MRC), Epoch:   2, Iter:   120, Loss: 0.020, F1: 0.833
Model: (ELECTRA-SL-MRC), Epoch:   2, Iter:   150, Loss: 0.013, F1: 0.615
Model: (ELECTRA-SL-MRC), Epoch:   2, Iter:   180, Loss: 0.009, F1: 0.857
Model: (ELECTRA-SL-MRC), Epoch:   2, Iter:   210, Loss: 0.001, F1: 1.000
Model: (ELECTRA-SL-MRC), Epoch:   2, Iter:   240, Loss: 0.023, F1: 0.556
Model: (ELECTRA-SL-MRC), Epoch:   2, Iter:   270, Loss: 0.014, F1: 0.889
Model: (ELECTRA-SL-MRC), Epoch:   2, Iter:   300, Loss: 0.009, F1: 0.800
Model: (ELECTRA-SL-MRC), Epoch:   2, Iter:   330, Loss: 0.025, F1: 0.500



VALIDATING

Model: (ELECTRA-SL-MRC), Epoch:   2, Iter:     0, Loss: 0.015
Model: (ELECTRA-SL-MRC), Epoch:   2, Iter:    30, Loss: 0.066
Model: (ELECTRA-SL-MRC), Epoch:   2, Iter:    60, Loss: 0.016
Model: (ELECTRA-SL-MRC), Epoch:   2, Iter:    90, Loss: 0.034
Model: (ELECTRA-SL-MRC), Epoch:   2, Iter:   120, Loss: 0.020
Model: (ELECTRA-SL-MRC), Epoch:   2, Iter:   150, Loss: 0.021
Model: (ELECTRA-SL-MRC), Epoch:   2, Iter:   180, Loss: 0.003
Model: (ELECTRA-SL-MRC), Epoch:   2, Iter:   210, Loss: 0.006
Model: (ELECTRA-SL-MRC), Epoch:   2, Iter:   240, Loss: 0.031
Model: (ELECTRA-SL-MRC), Epoch:   2, Iter:   270, Loss: 0.072
Model: (ELECTRA-SL-MRC), Epoch:   2, Iter:   300, Loss: 0.006
Model: (ELECTRA-SL-MRC), Epoch:   2, Iter:   330, Loss: 0.005
Model: (ELECTRA-SL-MRC), Epoch:   2, Iter:   360, Loss: 0.022



TESTING

Model: (ELECTRA-SL-MRC), Epoch:   2, Iter:     0, Loss: 0.038
Model: (ELECTRA-SL-MRC), Epoch:   2, Iter:    30, Loss: 0.003
Model: (ELECTRA-SL-MRC), Epoch:   2, Iter:    60, Loss: 0.088
Model: (ELECTRA-SL-MRC), Epoch:   2, Iter:    90, Loss: 0.042
Model: (ELECTRA-SL-MRC), Epoch:   2, Iter:   120, Loss: 0.024
Model: (ELECTRA-SL-MRC), Epoch:   2, Iter:   150, Loss: 0.071
Model: (ELECTRA-SL-MRC), Epoch:   2, Iter:   180, Loss: 0.061
Model: (ELECTRA-SL-MRC), Epoch:   2, Iter:   210, Loss: 0.004
Model: (ELECTRA-SL-MRC), Epoch:   2, Iter:   240, Loss: 0.009
Model: (ELECTRA-SL-MRC), Epoch:   2, Iter:   270, Loss: 0.154
Model: (ELECTRA-SL-MRC), Epoch:   2, Iter:   300, Loss: 0.120
Model: (ELECTRA-SL-MRC), Epoch:   2, Iter:   330, Loss: 0.021
Model: (ELECTRA-SL-MRC), Epoch:   2, Iter:   360, Loss: 0.015
Model: (ELECTRA-SL-MRC), Epoch:   2, Iter:   390, Loss: 0.091
Model: (ELECTRA-SL-MRC), Epoch:   2, Iter:   420, Loss: 0.020
Model: (ELECTRA-SL-MRC), Epoch:   2, Iter:   450, Loss: 0.036
Model: (ELECTRA-SL-MRC), Epoch:   2, Iter:   480, Loss: 0.013



EPOCH SUMMARY

Model: (ELECTRA-SL-MRC), Epoch:   2, Mean Train Loss: 0.030, Mean Train F1: 0.739
Model: (ELECTRA-SL-MRC), Epoch:   2, Mean Validation Loss: 0.050, Validation F1: 0.605
Model: (ELECTRA-SL-MRC), Epoch:   2, Mean Test Loss: 0.048, Test F1: 0.484
Impatience Level: 1


TRAINING

Model: (ELECTRA-SL-MRC), Epoch:   3, Iter:     0, Loss: 0.011, F1: 0.750
Model: (ELECTRA-SL-MRC), Epoch:   3, Iter:    30, Loss: 0.018, F1: 0.750
Model: (ELECTRA-SL-MRC), Epoch:   3, Iter:    60, Loss: 0.023, F1: 0.800
Model: (ELECTRA-SL-MRC), Epoch:   3, Iter:    90, Loss: 0.010, F1: 0.727
Model: (ELECTRA-SL-MRC), Epoch:   3, Iter:   120, Loss: 0.002, F1: 1.000
Model: (ELECTRA-SL-MRC), Epoch:   3, Iter:   150, Loss: 0.008, F1: 0.632
Model: (ELECTRA-SL-MRC), Epoch:   3, Iter:   180, Loss: 0.008, F1: 0.824
Model: (ELECTRA-SL-MRC), Epoch:   3, Iter:   210, Loss: 0.001, F1: 1.000
Model: (ELECTRA-SL-MRC), Epoch:   3, Iter:   240, Loss: 0.017, F1: 0.667
Model: (ELECTRA-SL-MRC), Epoch:   3, Iter:   270, Loss: 0.009, F1: 0.947
Model: (ELECTRA-SL-MRC), Epoch:   3, Iter:   300, Loss: 0.032, F1: 0.600
Model: (ELECTRA-SL-MRC), Epoch:   3, Iter:   330, Loss: 0.001, F1: 1.000



VALIDATING

Model: (ELECTRA-SL-MRC), Epoch:   3, Iter:     0, Loss: 0.023
Model: (ELECTRA-SL-MRC), Epoch:   3, Iter:    30, Loss: 0.011
Model: (ELECTRA-SL-MRC), Epoch:   3, Iter:    60, Loss: 0.012
Model: (ELECTRA-SL-MRC), Epoch:   3, Iter:    90, Loss: 0.065
Model: (ELECTRA-SL-MRC), Epoch:   3, Iter:   120, Loss: 0.003
Model: (ELECTRA-SL-MRC), Epoch:   3, Iter:   150, Loss: 0.013
Model: (ELECTRA-SL-MRC), Epoch:   3, Iter:   180, Loss: 0.017
Model: (ELECTRA-SL-MRC), Epoch:   3, Iter:   210, Loss: 0.104
Model: (ELECTRA-SL-MRC), Epoch:   3, Iter:   240, Loss: 0.008
Model: (ELECTRA-SL-MRC), Epoch:   3, Iter:   270, Loss: 0.067
Model: (ELECTRA-SL-MRC), Epoch:   3, Iter:   300, Loss: 0.023
Model: (ELECTRA-SL-MRC), Epoch:   3, Iter:   330, Loss: 0.018
Model: (ELECTRA-SL-MRC), Epoch:   3, Iter:   360, Loss: 0.022



TESTING

Model: (ELECTRA-SL-MRC), Epoch:   3, Iter:     0, Loss: 0.047
Model: (ELECTRA-SL-MRC), Epoch:   3, Iter:    30, Loss: 0.126
Model: (ELECTRA-SL-MRC), Epoch:   3, Iter:    60, Loss: 0.005
Model: (ELECTRA-SL-MRC), Epoch:   3, Iter:    90, Loss: 0.067
Model: (ELECTRA-SL-MRC), Epoch:   3, Iter:   120, Loss: 0.041
Model: (ELECTRA-SL-MRC), Epoch:   3, Iter:   150, Loss: 0.062
Model: (ELECTRA-SL-MRC), Epoch:   3, Iter:   180, Loss: 0.025
Model: (ELECTRA-SL-MRC), Epoch:   3, Iter:   210, Loss: 0.020
Model: (ELECTRA-SL-MRC), Epoch:   3, Iter:   240, Loss: 0.071
Model: (ELECTRA-SL-MRC), Epoch:   3, Iter:   270, Loss: 0.034
Model: (ELECTRA-SL-MRC), Epoch:   3, Iter:   300, Loss: 0.020
Model: (ELECTRA-SL-MRC), Epoch:   3, Iter:   330, Loss: 0.009
Model: (ELECTRA-SL-MRC), Epoch:   3, Iter:   360, Loss: 0.003
Model: (ELECTRA-SL-MRC), Epoch:   3, Iter:   390, Loss: 0.013
Model: (ELECTRA-SL-MRC), Epoch:   3, Iter:   420, Loss: 0.066
Model: (ELECTRA-SL-MRC), Epoch:   3, Iter:   450, Loss: 0.110
Model: (ELECTRA-SL-MRC), Epoch:   3, Iter:   480, Loss: 0.034



EPOCH SUMMARY

Model: (ELECTRA-SL-MRC), Epoch:   3, Mean Train Loss: 0.024, Mean Train F1: 0.770
Model: (ELECTRA-SL-MRC), Epoch:   3, Mean Validation Loss: 0.055, Validation F1: 0.603
Model: (ELECTRA-SL-MRC), Epoch:   3, Mean Test Loss: 0.049, Test F1: 0.489
Impatience Level: 2


TRAINING

Model: (ELECTRA-SL-MRC), Epoch:   4, Iter:     0, Loss: 0.008, F1: 0.667
Model: (ELECTRA-SL-MRC), Epoch:   4, Iter:    30, Loss: 0.001, F1: 1.000
Model: (ELECTRA-SL-MRC), Epoch:   4, Iter:    60, Loss: 0.019, F1: 0.533
Model: (ELECTRA-SL-MRC), Epoch:   4, Iter:    90, Loss: 0.006, F1: 0.952
Model: (ELECTRA-SL-MRC), Epoch:   4, Iter:   120, Loss: 0.006, F1: 0.909
Model: (ELECTRA-SL-MRC), Epoch:   4, Iter:   150, Loss: 0.008, F1: 0.909
Model: (ELECTRA-SL-MRC), Epoch:   4, Iter:   180, Loss: 0.003, F1: 0.875
Model: (ELECTRA-SL-MRC), Epoch:   4, Iter:   210, Loss: 0.016, F1: 0.500
Model: (ELECTRA-SL-MRC), Epoch:   4, Iter:   240, Loss: 0.019, F1: 0.600
Model: (ELECTRA-SL-MRC), Epoch:   4, Iter:   270, Loss: 0.001, F1: 1.000
Model: (ELECTRA-SL-MRC), Epoch:   4, Iter:   300, Loss: 0.002, F1: 0.800
Model: (ELECTRA-SL-MRC), Epoch:   4, Iter:   330, Loss: 0.010, F1: 0.909



VALIDATING

Model: (ELECTRA-SL-MRC), Epoch:   4, Iter:     0, Loss: 0.000
Model: (ELECTRA-SL-MRC), Epoch:   4, Iter:    30, Loss: 0.156
Model: (ELECTRA-SL-MRC), Epoch:   4, Iter:    60, Loss: 0.007
Model: (ELECTRA-SL-MRC), Epoch:   4, Iter:    90, Loss: 0.047
Model: (ELECTRA-SL-MRC), Epoch:   4, Iter:   120, Loss: 0.061
Model: (ELECTRA-SL-MRC), Epoch:   4, Iter:   150, Loss: 0.005
Model: (ELECTRA-SL-MRC), Epoch:   4, Iter:   180, Loss: 0.159
Model: (ELECTRA-SL-MRC), Epoch:   4, Iter:   210, Loss: 0.000
Model: (ELECTRA-SL-MRC), Epoch:   4, Iter:   240, Loss: 0.109
Model: (ELECTRA-SL-MRC), Epoch:   4, Iter:   270, Loss: 0.071
Model: (ELECTRA-SL-MRC), Epoch:   4, Iter:   300, Loss: 0.001
Model: (ELECTRA-SL-MRC), Epoch:   4, Iter:   330, Loss: 0.000
Model: (ELECTRA-SL-MRC), Epoch:   4, Iter:   360, Loss: 0.000



TESTING

Model: (ELECTRA-SL-MRC), Epoch:   4, Iter:     0, Loss: 0.034
Model: (ELECTRA-SL-MRC), Epoch:   4, Iter:    30, Loss: 0.111
Model: (ELECTRA-SL-MRC), Epoch:   4, Iter:    60, Loss: 0.166
Model: (ELECTRA-SL-MRC), Epoch:   4, Iter:    90, Loss: 0.069
Model: (ELECTRA-SL-MRC), Epoch:   4, Iter:   120, Loss: 0.031
Model: (ELECTRA-SL-MRC), Epoch:   4, Iter:   150, Loss: 0.001
Model: (ELECTRA-SL-MRC), Epoch:   4, Iter:   180, Loss: 0.181
Model: (ELECTRA-SL-MRC), Epoch:   4, Iter:   210, Loss: 0.278
Model: (ELECTRA-SL-MRC), Epoch:   4, Iter:   240, Loss: 0.001
Model: (ELECTRA-SL-MRC), Epoch:   4, Iter:   270, Loss: 0.013
Model: (ELECTRA-SL-MRC), Epoch:   4, Iter:   300, Loss: 0.026
Model: (ELECTRA-SL-MRC), Epoch:   4, Iter:   330, Loss: 0.069
Model: (ELECTRA-SL-MRC), Epoch:   4, Iter:   360, Loss: 0.024
Model: (ELECTRA-SL-MRC), Epoch:   4, Iter:   390, Loss: 0.042
Model: (ELECTRA-SL-MRC), Epoch:   4, Iter:   420, Loss: 0.050
Model: (ELECTRA-SL-MRC), Epoch:   4, Iter:   450, Loss: 0.028
Model: (ELECTRA-SL-MRC), Epoch:   4, Iter:   480, Loss: 0.003



EPOCH SUMMARY

Model: (ELECTRA-SL-MRC), Epoch:   4, Mean Train Loss: 0.017, Mean Train F1: 0.830
Model: (ELECTRA-SL-MRC), Epoch:   4, Mean Validation Loss: 0.065, Validation F1: 0.619
Model: (ELECTRA-SL-MRC), Epoch:   4, Mean Test Loss: 0.063, Test F1: 0.492
Impatience Level: 0


Checkpoint Created!


TRAINING

Model: (ELECTRA-SL-MRC), Epoch:   5, Iter:     0, Loss: 0.028, F1: 0.833
Model: (ELECTRA-SL-MRC), Epoch:   5, Iter:    30, Loss: 0.006, F1: 0.769
Model: (ELECTRA-SL-MRC), Epoch:   5, Iter:    60, Loss: 0.015, F1: 0.737
Model: (ELECTRA-SL-MRC), Epoch:   5, Iter:    90, Loss: 0.000, F1: 1.000
Model: (ELECTRA-SL-MRC), Epoch:   5, Iter:   120, Loss: 0.009, F1: 1.000
Model: (ELECTRA-SL-MRC), Epoch:   5, Iter:   150, Loss: 0.003, F1: 1.000
Model: (ELECTRA-SL-MRC), Epoch:   5, Iter:   180, Loss: 0.015, F1: 0.533
Model: (ELECTRA-SL-MRC), Epoch:   5, Iter:   210, Loss: 0.003, F1: 1.000
Model: (ELECTRA-SL-MRC), Epoch:   5, Iter:   240, Loss: 0.012, F1: 0.957
Model: (ELECTRA-SL-MRC), Epoch:   5, Iter:   270, Loss: 0.015, F1: 0.667
Model: (ELECTRA-SL-MRC), Epoch:   5, Iter:   300, Loss: 0.000, F1: 1.000
Model: (ELECTRA-SL-MRC), Epoch:   5, Iter:   330, Loss: 0.001, F1: 1.000



VALIDATING

Model: (ELECTRA-SL-MRC), Epoch:   5, Iter:     0, Loss: 0.012
Model: (ELECTRA-SL-MRC), Epoch:   5, Iter:    30, Loss: 0.026
Model: (ELECTRA-SL-MRC), Epoch:   5, Iter:    60, Loss: 0.010
Model: (ELECTRA-SL-MRC), Epoch:   5, Iter:    90, Loss: 0.021
Model: (ELECTRA-SL-MRC), Epoch:   5, Iter:   120, Loss: 0.001
Model: (ELECTRA-SL-MRC), Epoch:   5, Iter:   150, Loss: 0.009
Model: (ELECTRA-SL-MRC), Epoch:   5, Iter:   180, Loss: 0.039
Model: (ELECTRA-SL-MRC), Epoch:   5, Iter:   210, Loss: 0.004
Model: (ELECTRA-SL-MRC), Epoch:   5, Iter:   240, Loss: 0.064
Model: (ELECTRA-SL-MRC), Epoch:   5, Iter:   270, Loss: 0.006
Model: (ELECTRA-SL-MRC), Epoch:   5, Iter:   300, Loss: 0.001
Model: (ELECTRA-SL-MRC), Epoch:   5, Iter:   330, Loss: 0.003
Model: (ELECTRA-SL-MRC), Epoch:   5, Iter:   360, Loss: 0.955



TESTING

Model: (ELECTRA-SL-MRC), Epoch:   5, Iter:     0, Loss: 0.053
Model: (ELECTRA-SL-MRC), Epoch:   5, Iter:    30, Loss: 0.009
Model: (ELECTRA-SL-MRC), Epoch:   5, Iter:    60, Loss: 0.136
Model: (ELECTRA-SL-MRC), Epoch:   5, Iter:    90, Loss: 0.103
Model: (ELECTRA-SL-MRC), Epoch:   5, Iter:   120, Loss: 0.030
Model: (ELECTRA-SL-MRC), Epoch:   5, Iter:   150, Loss: 0.014
Model: (ELECTRA-SL-MRC), Epoch:   5, Iter:   180, Loss: 0.013
Model: (ELECTRA-SL-MRC), Epoch:   5, Iter:   210, Loss: 0.059
Model: (ELECTRA-SL-MRC), Epoch:   5, Iter:   240, Loss: 0.228
Model: (ELECTRA-SL-MRC), Epoch:   5, Iter:   270, Loss: 0.033
Model: (ELECTRA-SL-MRC), Epoch:   5, Iter:   300, Loss: 0.143
Model: (ELECTRA-SL-MRC), Epoch:   5, Iter:   330, Loss: 0.076
Model: (ELECTRA-SL-MRC), Epoch:   5, Iter:   360, Loss: 0.055
Model: (ELECTRA-SL-MRC), Epoch:   5, Iter:   390, Loss: 0.048
Model: (ELECTRA-SL-MRC), Epoch:   5, Iter:   420, Loss: 0.017
Model: (ELECTRA-SL-MRC), Epoch:   5, Iter:   450, Loss: 0.198
Model: (ELECTRA-SL-MRC), Epoch:   5, Iter:   480, Loss: 0.074



EPOCH SUMMARY

Model: (ELECTRA-SL-MRC), Epoch:   5, Mean Train Loss: 0.012, Mean Train F1: 0.882
Model: (ELECTRA-SL-MRC), Epoch:   5, Mean Validation Loss: 0.072, Validation F1: 0.617
Model: (ELECTRA-SL-MRC), Epoch:   5, Mean Test Loss: 0.073, Test F1: 0.459
Impatience Level: 1


TRAINING

Model: (ELECTRA-SL-MRC), Epoch:   6, Iter:     0, Loss: 0.010, F1: 0.933
Model: (ELECTRA-SL-MRC), Epoch:   6, Iter:    30, Loss: 0.001, F1: 1.000
Model: (ELECTRA-SL-MRC), Epoch:   6, Iter:    60, Loss: 0.002, F1: 0.833
Model: (ELECTRA-SL-MRC), Epoch:   6, Iter:    90, Loss: 0.006, F1: 0.714
Model: (ELECTRA-SL-MRC), Epoch:   6, Iter:   120, Loss: 0.001, F1: 1.000
Model: (ELECTRA-SL-MRC), Epoch:   6, Iter:   150, Loss: 0.001, F1: 1.000
Model: (ELECTRA-SL-MRC), Epoch:   6, Iter:   180, Loss: 0.005, F1: 0.667
Model: (ELECTRA-SL-MRC), Epoch:   6, Iter:   210, Loss: 0.000, F1: 1.000
Model: (ELECTRA-SL-MRC), Epoch:   6, Iter:   240, Loss: 0.011, F1: 1.000
Model: (ELECTRA-SL-MRC), Epoch:   6, Iter:   270, Loss: 0.001, F1: 1.000
Model: (ELECTRA-SL-MRC), Epoch:   6, Iter:   300, Loss: 0.011, F1: 0.900
Model: (ELECTRA-SL-MRC), Epoch:   6, Iter:   330, Loss: 0.001, F1: 1.000



VALIDATING

Model: (ELECTRA-SL-MRC), Epoch:   6, Iter:     0, Loss: 0.077
Model: (ELECTRA-SL-MRC), Epoch:   6, Iter:    30, Loss: 0.048
Model: (ELECTRA-SL-MRC), Epoch:   6, Iter:    60, Loss: 0.015
Model: (ELECTRA-SL-MRC), Epoch:   6, Iter:    90, Loss: 0.019
Model: (ELECTRA-SL-MRC), Epoch:   6, Iter:   120, Loss: 0.074
Model: (ELECTRA-SL-MRC), Epoch:   6, Iter:   150, Loss: 0.418
Model: (ELECTRA-SL-MRC), Epoch:   6, Iter:   180, Loss: 0.003
Model: (ELECTRA-SL-MRC), Epoch:   6, Iter:   210, Loss: 0.125
Model: (ELECTRA-SL-MRC), Epoch:   6, Iter:   240, Loss: 0.043
Model: (ELECTRA-SL-MRC), Epoch:   6, Iter:   270, Loss: 0.418
Model: (ELECTRA-SL-MRC), Epoch:   6, Iter:   300, Loss: 0.016
Model: (ELECTRA-SL-MRC), Epoch:   6, Iter:   330, Loss: 0.000
Model: (ELECTRA-SL-MRC), Epoch:   6, Iter:   360, Loss: 0.005



TESTING

Model: (ELECTRA-SL-MRC), Epoch:   6, Iter:     0, Loss: 0.022
Model: (ELECTRA-SL-MRC), Epoch:   6, Iter:    30, Loss: 0.077
Model: (ELECTRA-SL-MRC), Epoch:   6, Iter:    60, Loss: 0.008
Model: (ELECTRA-SL-MRC), Epoch:   6, Iter:    90, Loss: 0.140
Model: (ELECTRA-SL-MRC), Epoch:   6, Iter:   120, Loss: 0.061
Model: (ELECTRA-SL-MRC), Epoch:   6, Iter:   150, Loss: 0.067
Model: (ELECTRA-SL-MRC), Epoch:   6, Iter:   180, Loss: 0.134
Model: (ELECTRA-SL-MRC), Epoch:   6, Iter:   210, Loss: 0.045
Model: (ELECTRA-SL-MRC), Epoch:   6, Iter:   240, Loss: 0.013
Model: (ELECTRA-SL-MRC), Epoch:   6, Iter:   270, Loss: 0.020
Model: (ELECTRA-SL-MRC), Epoch:   6, Iter:   300, Loss: 0.073
Model: (ELECTRA-SL-MRC), Epoch:   6, Iter:   330, Loss: 0.085
Model: (ELECTRA-SL-MRC), Epoch:   6, Iter:   360, Loss: 0.017
Model: (ELECTRA-SL-MRC), Epoch:   6, Iter:   390, Loss: 0.048
Model: (ELECTRA-SL-MRC), Epoch:   6, Iter:   420, Loss: 0.052
Model: (ELECTRA-SL-MRC), Epoch:   6, Iter:   450, Loss: 0.024
Model: (ELECTRA-SL-MRC), Epoch:   6, Iter:   480, Loss: 0.073



EPOCH SUMMARY

Model: (ELECTRA-SL-MRC), Epoch:   6, Mean Train Loss: 0.010, Mean Train F1: 0.905
Model: (ELECTRA-SL-MRC), Epoch:   6, Mean Validation Loss: 0.093, Validation F1: 0.550
Model: (ELECTRA-SL-MRC), Epoch:   6, Mean Test Loss: 0.086, Test F1: 0.433
Impatience Level: 2


TRAINING

Model: (ELECTRA-SL-MRC), Epoch:   7, Iter:     0, Loss: 0.007, F1: 0.857
Model: (ELECTRA-SL-MRC), Epoch:   7, Iter:    30, Loss: 0.012, F1: 0.769
Model: (ELECTRA-SL-MRC), Epoch:   7, Iter:    60, Loss: 0.000, F1: 1.000
Model: (ELECTRA-SL-MRC), Epoch:   7, Iter:    90, Loss: 0.003, F1: 1.000
Model: (ELECTRA-SL-MRC), Epoch:   7, Iter:   120, Loss: 0.002, F1: 0.667
Model: (ELECTRA-SL-MRC), Epoch:   7, Iter:   150, Loss: 0.000, F1: 1.000
Model: (ELECTRA-SL-MRC), Epoch:   7, Iter:   180, Loss: 0.001, F1: 1.000
Model: (ELECTRA-SL-MRC), Epoch:   7, Iter:   210, Loss: 0.007, F1: 0.286
Model: (ELECTRA-SL-MRC), Epoch:   7, Iter:   240, Loss: 0.006, F1: 0.857
Model: (ELECTRA-SL-MRC), Epoch:   7, Iter:   270, Loss: 0.000, F1: 1.000
Model: (ELECTRA-SL-MRC), Epoch:   7, Iter:   300, Loss: 0.008, F1: 0.889
Model: (ELECTRA-SL-MRC), Epoch:   7, Iter:   330, Loss: 0.003, F1: 0.875



VALIDATING

Model: (ELECTRA-SL-MRC), Epoch:   7, Iter:     0, Loss: 0.126
Model: (ELECTRA-SL-MRC), Epoch:   7, Iter:    30, Loss: 0.089
Model: (ELECTRA-SL-MRC), Epoch:   7, Iter:    60, Loss: 0.025
Model: (ELECTRA-SL-MRC), Epoch:   7, Iter:    90, Loss: 0.059
Model: (ELECTRA-SL-MRC), Epoch:   7, Iter:   120, Loss: 0.558
Model: (ELECTRA-SL-MRC), Epoch:   7, Iter:   150, Loss: 0.054
Model: (ELECTRA-SL-MRC), Epoch:   7, Iter:   180, Loss: 0.348
Model: (ELECTRA-SL-MRC), Epoch:   7, Iter:   210, Loss: 0.044
Model: (ELECTRA-SL-MRC), Epoch:   7, Iter:   240, Loss: 0.001
Model: (ELECTRA-SL-MRC), Epoch:   7, Iter:   270, Loss: 0.057
Model: (ELECTRA-SL-MRC), Epoch:   7, Iter:   300, Loss: 0.112
Model: (ELECTRA-SL-MRC), Epoch:   7, Iter:   330, Loss: 0.046
Model: (ELECTRA-SL-MRC), Epoch:   7, Iter:   360, Loss: 0.051



TESTING

Model: (ELECTRA-SL-MRC), Epoch:   7, Iter:     0, Loss: 0.042
Model: (ELECTRA-SL-MRC), Epoch:   7, Iter:    30, Loss: 0.005
Model: (ELECTRA-SL-MRC), Epoch:   7, Iter:    60, Loss: 0.165
Model: (ELECTRA-SL-MRC), Epoch:   7, Iter:    90, Loss: 0.113
Model: (ELECTRA-SL-MRC), Epoch:   7, Iter:   120, Loss: 0.036
Model: (ELECTRA-SL-MRC), Epoch:   7, Iter:   150, Loss: 0.155
Model: (ELECTRA-SL-MRC), Epoch:   7, Iter:   180, Loss: 0.090
Model: (ELECTRA-SL-MRC), Epoch:   7, Iter:   210, Loss: 0.002
Model: (ELECTRA-SL-MRC), Epoch:   7, Iter:   240, Loss: 0.057
Model: (ELECTRA-SL-MRC), Epoch:   7, Iter:   270, Loss: 0.117
Model: (ELECTRA-SL-MRC), Epoch:   7, Iter:   300, Loss: 0.127
Model: (ELECTRA-SL-MRC), Epoch:   7, Iter:   330, Loss: 0.009
Model: (ELECTRA-SL-MRC), Epoch:   7, Iter:   360, Loss: 0.010
Model: (ELECTRA-SL-MRC), Epoch:   7, Iter:   390, Loss: 0.130
Model: (ELECTRA-SL-MRC), Epoch:   7, Iter:   420, Loss: 0.867
Model: (ELECTRA-SL-MRC), Epoch:   7, Iter:   450, Loss: 0.042
Model: (ELECTRA-SL-MRC), Epoch:   7, Iter:   480, Loss: 0.009



EPOCH SUMMARY

Model: (ELECTRA-SL-MRC), Epoch:   7, Mean Train Loss: 0.010, Mean Train F1: 0.902
Model: (ELECTRA-SL-MRC), Epoch:   7, Mean Validation Loss: 0.095, Validation F1: 0.568
Model: (ELECTRA-SL-MRC), Epoch:   7, Mean Test Loss: 0.085, Test F1: 0.452
Impatience Level: 3


